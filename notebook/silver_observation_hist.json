{
	"name": "silver_observation_hist",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "6807f543-c94d-4315-895a-81cf4fe5d491"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"bronze_location = \"abfss://bronze@prsynapselab.dfs.core.windows.net/\"\r\n",
					"write_mode=\"overwrite\"\r\n",
					""
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"observationSchema = spark.read.json(bronze_location+\"historic_data/observation/year=2016/part-00000-680e113d-60c6-468f-ba37-ea5ffd426dfe.c000.json\").schema\r\n",
					"observation_df = spark.read.schema(observationSchema).json(bronze_location+\"historic_data/observation/*/*.json\")"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"observation_df.printSchema()"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(observation_df)"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"observation_df_selected = observation_df.select(\"id\",\"resourceType\",\"status\",\"issued\",\"subject\",\"encounter\",\"effectiveDateTime\",\"valueQuantity\",\"valueString\",\"code\",\"component\",\"category\") \\\r\n",
					"                        .toDF(*(\"observation_id\",\"resourceType\",\"status\",\"issued\",\"patient\",\"encounter\",\"effectiveDateTime\",\"valueQuantity\",\"valueString\",\"code\",\"component\",\"category\"))"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"observation_df_selected.createOrReplaceTempView(\"observation_df_selected\")"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\r\n",
					"import org.apache.spark.sql.types.{StructType,ArrayType}\r\n",
					"import org.apache.spark.sql.DataFrame\r\n",
					"import org.apache.spark.sql.Column\r\n",
					"import org.apache.spark.sql.functions.col\r\n",
					"import org.apache.spark.sql.functions.explode_outer\r\n",
					"\r\n",
					"def flattenDataFrame(df: DataFrame): DataFrame = {\r\n",
					"  val fields = df.schema.fields\r\n",
					"  val fieldNames = fields.map(x => x.name)\r\n",
					"\r\n",
					"  for (i <- fields.indices) {\r\n",
					"    val field = fields(i)\r\n",
					"    val fieldType = field.dataType\r\n",
					"    val fieldName = field.name\r\n",
					"    fieldType match {\r\n",
					"      case _: ArrayType =>\r\n",
					"        val fieldNamesExcludingArray = fieldNames.filter(_ != fieldName)\r\n",
					"        val fieldNamesAndExplode = fieldNamesExcludingArray ++ Array(\r\n",
					"          s\"explode_outer($fieldName) as $fieldName\"\r\n",
					"        )\r\n",
					"        val explodedDf = df.selectExpr(fieldNamesAndExplode: _*)\r\n",
					"        return flattenDataFrame(explodedDf)\r\n",
					"      case structType: StructType =>\r\n",
					"        val childFieldNames =\r\n",
					"          structType.fieldNames.map(childname => fieldName + \".\" + childname)\r\n",
					"        val newFieldNames = fieldNames.filter(_ != fieldName) ++ childFieldNames\r\n",
					"        import org.apache.spark.sql.functions.col\r\n",
					"\r\n",
					"        val renamedCols =\r\n",
					"          newFieldNames.map { x =>\r\n",
					"            col(x.toString).as(x.toString.replace(\".\", \"_\"))\r\n",
					"          }\r\n",
					"\r\n",
					"        val explodedDf = df.select(renamedCols: _*)\r\n",
					"        return flattenDataFrame(explodedDf)\r\n",
					"      case _ =>\r\n",
					"    }\r\n",
					"  }\r\n",
					"\r\n",
					"  df\r\n",
					"}"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Creating observation Main Table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"observation_df_selected.printSchema()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\r\n",
					"val observation_main_df = spark.sql(\"select * from observation_df_selected\").drop(\"code\",\"component\",\"category\");\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\r\n",
					"val observation_main_df_flattened = flattenDataFrame(observation_main_df)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					},
					"collapsed": false
				},
				"source": [
					"%%spark\r\n",
					"display(observation_main_df_flattened)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\r\n",
					"val silver_location = \"abfss://silver@prsynapselab.dfs.core.windows.net/\"\r\n",
					"\r\n",
					"observation_main_df_flattened.coalesce(200).write.format(\"delta\").option(\"path\", silver_location+\"observation_main\").saveAsTable(\"fhir.observation_main_hash\")\r\n",
					""
				],
				"execution_count": 45
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Creating observation Code Table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					},
					"collapsed": false
				},
				"source": [
					"%%spark\r\n",
					"val observation_code_df = spark.sql(\"select observation_id, code from observation_df_selected\")\r\n",
					"val observation_code_df_flattened = flattenDataFrame(observation_code_df)\r\n",
					"display(observation_code_df_flattened)"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\r\n",
					"val silver_location = \"abfss://silver@prsynapselab.dfs.core.windows.net/\"\r\n",
					"\r\n",
					"observation_code_df_flattened.coalesce(200).write.format(\"delta\").option(\"path\", silver_location+\"observation_code\").saveAsTable(\"fhir.observation_code_hash\")"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Creating observation component Table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					},
					"collapsed": false
				},
				"source": [
					"%%spark\r\n",
					"val observation_component_df = spark.sql(\"select observation_id, component from observation_df_selected\")\r\n",
					"val observation_component_df_flattened = flattenDataFrame(observation_component_df)\r\n",
					"display(observation_component_df_flattened)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\r\n",
					"val silver_location = \"abfss://silver@prsynapselab.dfs.core.windows.net/\"\r\n",
					"\r\n",
					"observation_component_df_flattened.coalesce(200).write.format(\"delta\").option(\"path\", silver_location+\"observation_component\").saveAsTable(\"fhir.observation_component_hash\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Creating observation category Table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					},
					"collapsed": false
				},
				"source": [
					"%%spark\r\n",
					"val observation_category_df = spark.sql(\"select observation_id, category from observation_df_selected\")\r\n",
					"val observation_category_df_flattened = flattenDataFrame(observation_category_df)\r\n",
					"display(observation_category_df_flattened)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\r\n",
					"val silver_location = \"abfss://silver@prsynapselab.dfs.core.windows.net/\"\r\n",
					"\r\n",
					"observation_category_df_flattened.coalesce(200).write.format(\"delta\").option(\"path\", silver_location+\"observation_category\").saveAsTable(\"fhir.observation_category_hash\")"
				],
				"execution_count": null
			}
		]
	}
}